## Inplace_operation

不修改内存地址的操作

如果使用 `X = X + Y`而不是 `X += Y`或 `X[:] = X + Y`，那么会创建一个新的张量对象，并将其引用赋给变量X，原来的X会被垃圾回收（除非有其他引用）

```python
Z = torch.zeros_like(Y)
print('id(Z):', id(Z))
Z[:] = X + Y
print('id(Z):', id(Z))
```

```python
before = id(X)
X += Y  # if X = X + Y, then id(X) != before
id(X) == before
```

## 举例

PyTorch中有很多就地操作（in-place operations），它们通常以下划线后缀 `_`结尾。以下是常见的就地操作：

> 注意：Y2和Y4的地址相同，加了下划线才是就地操作
>
> print('id(Y1):', id(Y))
>
> Y = torch.abs(Y)
>
> print('id(Y2):', id(Y))
>
> print('id(Y3):', id(torch.abs(Y)))
>
> print('id(Y4):', id(torch.abs_(Y)))

1. **基本算术运算** ：

* `tensor.add_(value)` - 就地加法
* `tensor.sub_(value)` - 就地减法
* `tensor.mul_(value)` - 就地乘法
* `tensor.div_(value)` - 就地除法

1. **高级数学操作** ：

* `tensor.pow_(exponent)` - 就地幂运算
* `tensor.abs_()` - 就地绝对值
* `tensor.sqrt_()` - 就地平方根
* `tensor.log_()` - 就地自然对数
* `tensor.exp_()` - 就地指数

1. **激活函数** ：

* `tensor.sigmoid_()` - 就地sigmoid激活
* `tensor.tanh_()` - 就地tanh激活
* `tensor.relu_()` - 就地ReLU激活

1. **归一化和标准化** ：

* `tensor.clamp_(min, max)` - 就地裁剪数值到指定范围
* `tensor.normalize_(p, dim)` - 就地归一化

1. **形状和维度操作** ：

* `tensor.transpose_(dim0, dim1)` - 就地转置指定维度
* `tensor.unsqueeze_(dim)` - 就地增加维度
* `tensor.squeeze_(dim)` - 就地减少维度

1. **数据类型转换** ：

* `tensor.float_()` - 就地转换为浮点型
* `tensor.int_()` - 就地转换为整型
* `tensor.long_()` - 就地转换为长整型

1. **其他运算符** ：

* `tensor.copy_(src)` - 就地复制另一个张量的值
* `tensor.fill_(value)` - 就地填充指定值
* `tensor.zero_()` - 就地填充零
* `tensor.random_()` - 就地填充随机值

1. **复合赋值运算符** ：

* `tensor += other` - 就地加法
* `tensor -= other` - 就地减法
* `tensor *= other` - 就地乘法
* `tensor /= other` - 就地除法

1. **切片赋值** ：

* `tensor[:] = value` - 就地修改所有元素
* `tensor[0:5] = value` - 就地修改部分元素

这些就地操作的主要优点是可以节省内存，因为它们不创建新的张量对象，而是直接修改现有张量的值。在训练大型神经网络或处理大量数据时，这种内存优化可能会带来显著的性能提升。
